{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9730985,"sourceType":"datasetVersion","datasetId":5955022}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **BBC News Classification Kaggle Mini-Project**\n\nWe want to start by importing the necessary libraries. In this notebook, we will use the methodology of Exploratory Data Analysis (EDA).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import floor\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom wordcloud import WordCloud\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1. Explanatory Data Analysis\n\n# Step 1.1 Load the Data","metadata":{}},{"cell_type":"code","source":"# import data, there are three files, i.e. sample solution, test and train\ntrain = pd.read_csv('/kaggle/input/bbc-news/BBC News Train.csv') # 3 columns, i.e., ArticleId, Text and Category, 1490 rows\ntest=pd.read_csv('/kaggle/input/bbc-news/BBC News Test.csv') # 2 cols, i.e., ArticleId, Text, 735 rows\nsol=pd.read_csv('/kaggle/input/bbc-news/BBC News Sample Solution.csv') # 2 cols, i.e. Article, Category, 735 rows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1.2 Insepct the Data\n\nAfter inspecting the data, we need to perform some generic processing, i.e. check for null values, etc.\n\nBefore we start, like dealing with numeric data, we would like to inspect potential null values and rows that might cause issues, after which we will hit the essence of this step.","metadata":{}},{"cell_type":"code","source":"# Inspect the data with .info(); since the data is not numeric, I will not use .describe()\nprint(train.info())\nprint(test.info())\nprint(sol.info())\n\n# Use .head() to see the first rows\nprint(train.head(10))\nprint(test.head(10))\nprint(sol.head(10))\n\n# Check for null values\nprint(train.isnull().sum())\nprint(test.isnull().sum())\nprint(sol.isnull().sum())\n\n# It seems that all is good!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1.3 Visualize the Data\n\nIn our case, we are dealing with textual data which we desire to categorize. We would like to explore the basic characteristics of the texts, among which, there are word counts and word length.","metadata":{}},{"cell_type":"code","source":"\ntrain['WordCount']=train['Text'].apply(lambda x:len(x.split()))\ntrain['AvgWordLength']=train['Text'].apply(lambda x: np.nan_to_num(sum(len(w) for w in x.split())/len(x.split())))\ntrain['CharCount']=train['Text'].apply(lambda x:sum(1 for c in x if c.isalnum()))\n#print(train.head(10))\ncols=['WordCount','AvgWordLength','CharCount']\ndiv=[25,5,200]\nfor i in [0,2]:\n    plt.figure(figsize=(10, 6))\n    print(floor(max(train[cols[i]])/div[i]))\n    plt.hist(train[cols[i]], bins=range(0, floor(max(train[cols[i]])) + 1, floor(max(train[cols[i]])/div[i])), edgecolor='blue')\n    plt.title('Dist of '+cols[i])\n    plt.xlabel(cols[i]+' Boxes')\n    plt.ylabel('Freq')\n    plt.xticks(range(0, floor(max(train[cols[i]])) + 1, floor(max(train[cols[i]])/div[i])))  # Adjust for clear labels\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1.4 Feature Engineering\n\nIn this step, we need to process the texts so that the machine can treat them. \n\nTo perform the procedure called feature engineering, first of all, it is advisable that the puntuations and stop words are removed, because we assume that they do not contribute much to the categorization learning process. Morever, we need some extra cleaning for the letter cases. Secondly, to rule out the effect from grammar, conjugation and different forms of the same root, we need the process of tokenization and lemmatization. Last but not least, in order that the machine can play with the data and extract the patterns out of it, it is required to convert the texts into numbers, which will allow matrix calculations and machine learning.","metadata":{}},{"cell_type":"code","source":"print(train.isnull().sum())\nprint(test.isnull().sum())\nprint(sol.isnull().sum())\n\n# It seems that everything is good!\n\nprint(sol['Category'].unique()) # 5 possibilities,['sport' 'tech' 'business' 'entertainment' 'politics']\n\n# Download the stopwords\n#nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# I looked up other people's work to know what libraries to use\n# Reference: https://www.kaggle.com/code/dazhengzhu/bbc-news-classification\ndef Processing(t):\n    # Tokenize \n    stemmer=PorterStemmer()\n    ws = nltk.word_tokenize(text=t)\n    # Filtering words and converting case\n    ws=[w.lower() for w in ws if w.isalpha()]\n    # Find roots for words that are not stopwords\n    sws=set(stopwords.words('english'))\n    ws=[stemmer.stem(w) for w in ws if w not in sws]\n    return ' '.join(ws)\n    \ntrain['ProcessedText']=train['Text'].apply(lambda x:Processing(x))\ntest['ProcessedText']=test['Text'].apply(lambda x:Processing(x))\n\n#print(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1.5 Data Exploration","metadata":{}},{"cell_type":"code","source":"# Reference: https://www.kaggle.com/code/dazhengzhu/bbc-news-classification\n# There is a very fun way to visualize the data! I found the library from the link above!\nt = train['ProcessedText'][0]\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(t)\nplt.figure(figsize=(11, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\n\n# With this method, we can play with the train and test data!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Part 2. Building and Fitting the Model**","metadata":{}},{"cell_type":"code","source":"# We apply TF-IDF vectorization (Frequency-Inverse Document Frequency)\ndef TfidfLearning(data,n=5):\n    vectorizer = TfidfVectorizer(stop_words='english') # there are many parameters that we can play with though\n    tfid_mat = vectorizer.fit_transform(data['ProcessedText'])\n    nmf = NMF(n_components=n, random_state=42)\n    W = nmf.fit_transform(tfid_mat) # document-topic mat\n    H=nmf.components_ # topic-term mat\n    return (tfid_mat,vectorizer,nmf,W,H)\n    \nn = len(sol['Category'].unique()) #5\ntfid_mat,vectorizer,nmf,W,H=TfidfLearning(train,n=5)\nprint(W.shape,H.shape,W,H,test)\n\n# We can manually assign labels by looking at the top words\ntopics = vectorizer.get_feature_names_out()\nm = 15\nfor i,ws in enumerate(H):\n    tops = [topics[i] for i in ws.argsort()[-m:]][::-1] # last 10 words and reverse the order\n    print(i,tops)\n\n# After observing the data, we can determine that 0 - technic, 1 - business, 2 - entertainment, 3 - sports, 4 - politics\n    \n# Alternatively, we can look at all the permutations to find out the permutation that gives the best accuracy\nlabels=np.array([i for i in range(n)]) # n=5\ncats=sol['Category'].unique()\naccuracy=0\nperm_f=None\n\n# Because to determine the labels, we only need an approximation, assuming that the data are good, we will approximate the\n# real category of a text to be the category/topic that has the highest weight in the document-topic matrix\ny_pred = np.argmax(W, axis=1)\nprint(test_sol,test_sol.shape,test.shape,sol.shape)\n\nfor perm in list(itertools.permutations(labels)):\n    y_true=[perm[np.where(cats==e)[0][0]] for e in train['Category']]\n    acc = sum(y_pred==y_true)/len(y_true)\n    if acc>accuracy:\n        accuracy=acc\n        perm_f=perm\n    #print(perm,acc)\nprint(\"The best accuracy and the best permutation are: \", accuracy,perm_f)\n\n#perm_f=[3,0,1,2,4]\nfor i in range(len(cats)):\n    #print(i,cats[perm_f[i]])\n    print(perm_f[i],cats[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can test this model on test data to see if the result is desirable; if not, we consider making some improvements.","metadata":{}},{"cell_type":"code","source":"test_sol = pd.merge(test, sol, on='ArticleId', how='left') \ntfid_mat_test= vectorizer.transform(test_sol['ProcessedText'])\nW_test=nmf.fit_transform(tfid_mat_test) \ny_pred_test = np.argmax(W_test, axis=1)\ny_true_test= np.array([perm_f[np.where(cats==e)[0][0]] for e in test_sol['Category']])\nacc = sum(y_pred_test==y_true_test)/len(y_true_test)\nprint(\"Without supervised learning, just approximating the category by finding maximum the document-topic weight, accuracy for test data:\",acc)\n\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\nmse = mean_squared_error(y_true_test, y_pred_test)\ncm = confusion_matrix(y_true, y_pred)\nprint(f\"The MSE for the test data is calculated to be: {mse}\")\nprint(\"Confusion Matrix:\",cm)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n\nplt.title('HM for CM for Test Data')\nplt.xlabel('Y_pred')\nplt.ylabel('Y_true')\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected the result is poor due to many limitations of NMF. Now we consider adjusting the hyperparameters to improve the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nvec = TfidfVectorizer()\nnmf = NMF()\n\n# Set the hyperparameter grid\ngrid = {\n    'vectorizer__max_df': [0.9, 0.95, 1.0],\n    'vectorizer__min_df': [1, 2, 5],\n    'nmf__init': ['random', 'nndsvd'],\n}\npipeline = Pipeline([\n    ('vectorizer', vec),\n    ('nmf', nmf)\n])\ngridSearch = GridSearchCV(pipeline, grid, cv=5, scoring='neg_mean_squared_error')\ngridSearch.fit(train['ProcessedText'], y_true)\n\n# Best parameters and score\nprint(\"The best parameters for vectorization and matrix factorization are:\", gridSearch.best_params_)\nprint(\"The corresponding mean squared error:\", -gridSearch.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have the best parameters, we will use them for our further investigations.","metadata":{}},{"cell_type":"markdown","source":"# **Part 3 Model Evaluation and Comparison with Supervised Learning**\n\nFrom Part 2, we From Part 2, we can know the mapping of numeric labels and the categories as follows:\n* 3 sport\n* 0 tech\n* 4 business\n* 2 entertainment\n* 1 politics\n\nWith this information, we can further perform supervised learning mechanisms such as logistic regression, in order to compare the results of unsupervised learning and those of supervised learning.","metadata":{}},{"cell_type":"code","source":"\n# Now we can try random forest\nrf = RandomForestClassifier(random_state=42)\nrf.fit(W, y_true)\n\ny_pred = rf.predict(W)\nacc = accuracy_score(y_pred, y_true)\nprint(f\"Accuracy for train data: {acc}\")\n\ny_pred_test = rf.predict(W_test)\nacc_test = accuracy_score(y_pred_test, y_true_test)\nprint(f\"Accuracy for test data: {acc_test}\")\n\n# Now we will use Logistic Regression\nmodel_log=LogisticRegression()    \nmodel_log.fit(W, y_true)\n\ny_pred = model_log.predict(W)\nacc = accuracy_score(y_pred, y_true)\nprint(f\"Accuracy for train data: {acc}\")\n\ny_pred_test = model_log.predict(W_test)\nacc_test = accuracy_score(y_pred_test, y_true_test)\nprint(f\"Accuracy for test data: {acc_test}\")\n\n# Linear SVC\nsvc = SVC(kernel='linear', random_state=42)\nsvc=svc.fit(W, y_true)\ny_pred = svc.predict(W)\nacc = accuracy_score(y_pred, y_true)\nprint(f\"Accuracy for train data: {acc}\")\n\ny_pred_test = svc.predict(W_test)\nacc_test = accuracy_score(y_pred_test, y_true_test)\nprint(f\"Accuracy for test data: {acc_test}\")\n\n\n# KNN\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(W, y_true)\ny_pred = knn.predict(W)\nacc = accuracy_score(y_pred, y_true)\nprint(f\"Accuracy for train data: {acc}\")\n\ny_pred_test = knn.predict(W_test)\nacc_test = accuracy_score(y_pred_test, y_true_test)\nprint(f\"Accuracy for test data: {acc_test}\")\n\n\n# Naive Bayes\nnb = MultinomialNB()\nnb.fit(W, y_true)\ny_pred = nb.predict(W)\nacc = accuracy_score(y_pred, y_true)\nprint(f\"Accuracy for train data: {acc}\")\n\ny_pred_test = nb.predict(W_test)\nacc_test = accuracy_score(y_pred_test, y_true_test)\nprint(f\"Accuracy for test data: {acc_test}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also check if the data size can affect the accuracies by varying the data sizes and redo the tests above.","metadata":{}},{"cell_type":"code","source":"for pc in [0.2,0.5,0.7,0.9,1]:\n    print(\"Now we are traing the model for this portion of training data: \",pc)\n    tfid_mat,vectorizer,nmf,W,H=TfidfLearning(train[:floor(len(train)*pc)],n=5)\n    y_true= np.array([perm_f[np.where(cats==e)[0][0]] for e in train[:floor(len(train)*pc)]['Category']])\n\n    test_sol = pd.merge(test, sol, on='ArticleId', how='left') \n    tfid_mat_test= vectorizer.transform(test_sol['ProcessedText'])\n    W_test=nmf.fit_transform(tfid_mat_test) \n    y_pred_test = np.argmax(W_test, axis=1)\n    y_true_test= np.array([perm_f[np.where(cats==e)[0][0]] for e in test_sol['Category']])\n    #acc = sum(y_pred_test==y_true_test)/len(y_true_test)\n    #print(\"Without supervised learning, just approximating the category by finding maximum the document-topic weight, accuracy for test data:\",acc)\n\n    # Now we can try random forest\n    rf = RandomForestClassifier(random_state=42)\n    rf.fit(W, y_true)\n\n    y_pred = rf.predict(W)\n    acc = accuracy_score(y_pred, y_true)\n    print(f\"Accuracy for train data: {acc}\")\n\n    y_pred_test = rf.predict(W_test)\n    acc_test = accuracy_score(y_pred_test, y_true_test)\n    print(f\"Accuracy for test data: {acc_test}\")\n\n    # Now we will use Logistic Regression\n    model_log=LogisticRegression()    \n    model_log.fit(W, y_true)\n\n    y_pred = model_log.predict(W)\n    acc = accuracy_score(y_pred, y_true)\n    print(f\"Accuracy for train data: {acc}\")\n\n    y_pred_test = model_log.predict(W_test)\n    acc_test = accuracy_score(y_pred_test, y_true_test)\n    print(f\"Accuracy for test data: {acc_test}\")\n\n    # Linear SVC\n    svc = SVC(kernel='linear', random_state=42)\n    svc=svc.fit(W, y_true)\n    y_pred = svc.predict(W)\n    acc = accuracy_score(y_pred, y_true)\n    print(f\"Accuracy for train data: {acc}\")\n\n    y_pred_test = svc.predict(W_test)\n    acc_test = accuracy_score(y_pred_test, y_true_test)\n    print(f\"Accuracy for test data: {acc_test}\")\n\n\n    # KNN\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(W, y_true)\n    y_pred = knn.predict(W)\n    acc = accuracy_score(y_pred, y_true)\n    print(f\"Accuracy for train data: {acc}\")\n\n    y_pred_test = knn.predict(W_test)\n    acc_test = accuracy_score(y_pred_test, y_true_test)\n    print(f\"Accuracy for test data: {acc_test}\")\n\n\n    # Naive Bayes\n    nb = MultinomialNB()\n    nb.fit(W, y_true)\n    y_pred = nb.predict(W)\n    acc = accuracy_score(y_pred, y_true)\n    print(f\"Accuracy for train data: {acc}\")\n\n    y_pred_test = nb.predict(W_test)\n    acc_test = accuracy_score(y_pred_test, y_true_test)\n    print(f\"Accuracy for test data: {acc_test}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Conclusion**\n\nThere are two observations from the results:\n1. The size of training data does not have a significant impact on the accuracies\n1. Among the supervised learning mechanisms, KNN seems to yield the best result for the test data, whilst Naive Bayes is the worst. \n1. In general, even if a model can fit the training data well, the prediction for new data can be poor, which indicates that some improvements might need to be done on the model and more data might be needed. This is understandable, since we use use a small fraction of the data to train the models, which definitely leads to huge bias and uncertainty. For future reference, in order to make the model work, first and foremost, we need to increase the data size and combine the strategy of Tfidf and the KNN mechanism.","metadata":{}}]}