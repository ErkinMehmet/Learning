{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13840030,"sourceType":"datasetVersion","datasetId":8814766}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenizer\n\nWe discuss on a word-wise tokenizer; however, it is possible to do Byte Pair Encoding (like in GPT models)","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/texttoanalyse/the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n    raw_text=f.read()\nprint (f\"Nombre total de caractères {len(raw_text)} et les premiers 100 cars {raw_text[:99]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:08:49.410137Z","iopub.execute_input":"2025-11-23T15:08:49.410952Z","iopub.status.idle":"2025-11-23T15:08:49.429375Z","shell.execute_reply.started":"2025-11-23T15:08:49.410907Z","shell.execute_reply":"2025-11-23T15:08:49.428424Z"}},"outputs":[{"name":"stdout","text":"Nombre total de caractères 20781 et les premiers 100 cars THE VERDICT\nJune 1908\n\nI had always thought Jack Gisburn rather a cheap genius--though a\n\ngood fell\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import re\nraw_tokens=re.split(r'([\\n]|[,.:;?_!\"()\\']|--|\\s+)',raw_text) # split the text but include the splitters\nraw_tokens=[t for t in raw_tokens if t.strip()] # get rid of empty strings; we can keep the whitespaces if the text should be sensitive to indentation and spaces\nprint(f\"{len(raw_tokens)} {raw_tokens[:30]}\")\n# generate token ids (int) for unique tokens\n# sort the unique tokens alphabetically and get the vocab size\nuniq_tokens=sorted(set(raw_tokens))\nprint(f\"size {len(uniq_tokens)}\")\n# build the vocab\nvocab={token:inx for inx,token in enumerate(uniq_tokens)}\n\nfor i,item in enumerate(vocab.items()):\n    print(item)\n    if i>=10:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:32:15.833615Z","iopub.execute_input":"2025-11-23T15:32:15.833912Z","iopub.status.idle":"2025-11-23T15:32:15.845038Z","shell.execute_reply.started":"2025-11-23T15:32:15.833893Z","shell.execute_reply":"2025-11-23T15:32:15.843940Z"}},"outputs":[{"name":"stdout","text":"4667 ['THE', 'VERDICT', 'June', '1908', 'I', 'had', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to']\nsize 1148\n('!', 0)\n('\"', 1)\n(\"'\", 2)\n('(', 3)\n(')', 4)\n(',', 5)\n('--', 6)\n('.', 7)\n('1908', 8)\n(':', 9)\n(';', 10)\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"Later with the vocabulary, we can create a Tokenizer class that has a decode method and a decode method, which converts ids into tokens and vice versa.\n\nConsiderations:\n1. Unknown vocabulary: use a diverse training dataset to extend the vocab; we can use a **special context token** for unknown words\n2. End of text is interesting to consider when several text sources are used, which means the previous text ends and the new text starts => **endoftext** token\n3. Other tokens interesting to consider: [BOS] (beginning of sequence: start of a text), [EOS] (end of sequence: end of a text, similar to [endoftext]), [PAD] (padding: when training LLMs with batch sizes larger than one, the shorter texts are extended of padded with the token)\n\n\nFor GPT, only [endoftext] token is used for simplicity (no other tokens are used). For unknown words, GPT model breaks down unknown words into subword units. (because its tokenizer uses Byte Pair Encoding)","metadata":{}},{"cell_type":"code","source":"class SimpleTokenizerV1: # word wise tokenizer\n    def __init__(self,vocab):\n        self.str_to_int=vocab\n        self.int_to_str={i:s for s,i in vocab.items()}\n    def encode(self,text):\n        preprocessed=re.split(r'([\\n]|[,.:;?_!\"()\\']|--|\\s+)',text)\n        preprocessed=[item.strip() for item in preprocessed if item.strip()]\n        ids=[self.str_to_int[s] for s in preprocessed]\n        return ids;\n    def decode(self,ids):\n        text=\" \".join([self.int_to_str[id] for id in ids])\n        text=re.sub(r'\\s+([,.:;?_!\"()\\'])',r'\\1',text) # get rid of the extra space before punctuations\n        return text\n\n\n# special context token: <|unk|> => now the tokenizer can handler unknown words\n# <|endoftext|>: useful when we use several text sources\nuniq_tokens=sorted(set(raw_tokens))\nif \"<|endoftext|>\" not in uniq_tokens:\n    uniq_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n    vocab={token:inx for inx,token in enumerate(uniq_tokens)}\n\nclass SimpleTokenizerV2: # word wise tokenizer with special tokens\n    def __init__(self,vocab):\n        self.str_to_int=vocab\n        self.int_to_str={i:s for s,i in vocab.items()}\n    def encode(self,text):\n        preprocessed=re.split(r'([\\n]|[,.:;?_!\"()\\']|--|\\s+)',text)\n        preprocessed=[item.strip() for item in preprocessed if item.strip()]\n        preprocessed=[item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n        ids=[self.str_to_int[s] for s in preprocessed]\n        return ids;\n    def decode(self,ids):\n        text=\" \".join([self.int_to_str[id] for id in ids])\n        text=re.sub(r'\\s+([,.:;?_!\"()\\'])',r'\\1',text) # get rid of the extra space before punctuations\n        return text\n\ndef joinTexts(texts):\n    return \" <|endoftext|> \".join(texts) # add end of text token between texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:57:39.142034Z","iopub.execute_input":"2025-11-23T15:57:39.142911Z","iopub.status.idle":"2025-11-23T15:57:39.154209Z","shell.execute_reply.started":"2025-11-23T15:57:39.142877Z","shell.execute_reply":"2025-11-23T15:57:39.153060Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"# applicaiton of the tokenizer to convert a text to a list of ids\ntokenizer=SimpleTokenizerV2(vocab)\nids=tokenizer.encode(raw_text)\ndecoded_text=tokenizer.decode(ids)\nprint(ids[:20])\nprint(decoded_text[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:56:00.709506Z","iopub.execute_input":"2025-11-23T15:56:00.709761Z","iopub.status.idle":"2025-11-23T15:56:00.720579Z","shell.execute_reply.started":"2025-11-23T15:56:00.709734Z","shell.execute_reply":"2025-11-23T15:56:00.719246Z"}},"outputs":[{"name":"stdout","text":"[105, 118, 65, 8, 59, 532, 166, 1021, 63, 42, 837, 132, 272, 503, 6, 1020, 132, 517, 452, 409]\nTHE VERDIC\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"Byte Pair Encoding\n\nUsed in GPT2, GPT3 and the original model","metadata":{}},{"cell_type":"code","source":"!pip install tiktoken # BPE encoder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import importlib\nimport tiktoken\nprint(\"tiktoken vers:\", importlib.metadata.version(\"tiktoken\"))\n\ntokenizer=tiktoken.get_encoding(\"gpt2\") # charge the tokenizer for gpt2\ntext=(\"hello i am good. <|endoftext|>\"\n      \"hello how are you? <|endoftext|>\"\n     )# having one special token\nints=tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\nprint(ints[:10])\nprint(tokenizer.decode(ints))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creation of Input-Target Pairs\n\nWe can use data loader that fetches the input-target pairs using a sliding window approach\n\nWe can create x and y which contain respectively the tokens and the targets.","metadata":{}},{"cell_type":"code","source":"# we can use the ints in the window above as the encoded token ids\ncontext_size=4\nfor i in range(1,context_size+1):\n    context=ints[:i]\n    desired=ints[i]\n    print(f\"{context}->{desired}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self,txt,tokenizer,max_length,stride):\n        self.input_ids=[]\n        self.target_ids=[]\n        # tokenize the entire text\n        token_ids=tokenizer.encode(txt,allowed_special=\"<|endoftext|>\")\n        for i in range(0,len(token_ids)-max_length,stride):\n            input_chunk=token_ids[i:i+max_length]\n            target_chunk=token_ids[i+1,i+max_length+1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n    def __len__(self): # len()\n        return len(self.input_ids)\n    def __getitem__(self,idx): # []\n        return self.input_ids[idx],self.target_ids[idx]        \n        \ndef create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n    tokenizer=tiktoken.get_encoding(\"gpt2\")\n    dataset=GPTDatasetV1(txt,tokenizer,max_length,stride)\n    dataloader=DataLoader(\n        dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers\n    ) # num_workers = num of CPUs\n    # drop_last: if the last batch does not have enough elements, then it is dropped\n    return dataloader\n\nimport torch\nprint(\"pytorch version:\",torch.__version__)\ndataloader=create_dataloader_v1(raw_text,batch_size=1,max_length=4,stride=1,shuffle=False)\ndata_iter=iter(dataloader)\nfirst_batch=next(data_iter)\nprint(\"first input\",first_batch) # is a pair tensor matrices of input + target\n# each tensor object is an array of batch_size row(s) and max_length columns(s)\nsnd_batch=next(data_iter)\nprint(\"second input\",snd_batch)\n# each adjacent rows differ by position of 1 becasue the stride is set to be 1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In DL, small batch size (a hyperparam) require less memory during training but lead to more noises","metadata":{}},{"cell_type":"code","source":"# set stride=4 to no skip words and not make inputs overlap (it can help reduce overfitting)\ndataloader=create_dataloader_v1(raw_text,batch_size=8,max_length=4,stride=4,shuffle=False) \n# each iterated input or target has now 8 rows (batch_size) and 4 columns (max_length)\n# we augmented the batch_size to reduce noise but it will increase the memory cost\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Token Embeddings","metadata":{}},{"cell_type":"code","source":"!pip install gensim\n# many big companies have trained word embeddings => for example, google has Word2Vec ready to used. They\n# provide pretrained vectors trained on Google News dataset of about 100 B words\n# the model contains 300-dimensional vectors for 3M words are phrases","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gensim.downloader as api\nmodel=api.load(\"word2vec-google-news-300\")\nword_vectors=model\nprint(word_vectors['computer']) # a vector having 300 values\nprint(word_vectors.most_similar(positive=['king','woman'],negative=['man'],topn=10)) \n# gives sorted results for king+woman-man\nprint(word_vectors.similarity('woman','man')) # gives similarity\nprint(word_vectors.most_similar(\"tower\",topn=5))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creation of Token Embeddings","metadata":{}},{"cell_type":"code","source":"# example text: quick fox is in the house\ninput_ids=torch.tensor([2,3,5,1])\nvocab_size=6\noutput_dim=3\ntorch.manual_seed(123)\nembedding_layer=torch.nn.Embedding(vocab_size,outpout_dim) # init embedding weights\n# nn.Embedding is preferred compared to nn.Linear because it is more computationally efficient\nprint(embedding_layer.weights) # the weights are initialized\n\nprint(embedding_layer(torch.tensor([3]))) # print the weights for the token with token_id=3 in vocab\nprint(embedding_layer(input_ids)) # produce the matrix with weights for all the tokens => num of inputs x dim\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Positional embeddings","metadata":{}},{"cell_type":"code","source":"vocab_size=50257\nembed_dim=256\nembedding_layer=torch.nn.Embedding(vocab_size,embed_dim) \n\nmax_length=4\ndataloader=create_dataloader_v1(\n    raw_text,batch_size=8,max_length=max_length,stride=max_length,shuffle=False\n)\ndata_iter=iter(dataloader)\ninputs,targets=next(data_iter) # size of 8 x 4 (size of batch x context length)\nprint(inputs.shape)\n# after tokenization, we should have a 8 x 4 x 256 tensor (256 is the dim of a token vector)\ntoken_embeddings=token_embedding_layer(inputs) # 8x4x256 the new dimension is added to the right! \n#Because it is a newly added dim (to be distinguished from broadcasting)\nprint(token_embeddings.shape)\n# we need to add positional embeddings => max_length=4, so we only have 4 possibilities for the position\n# each positional embedding has a size of 256 (token dim) => positional embedding matrix size = 4 x 256\npos_embedding_layer=torch.nn.Embedding(max_length,embed_dim) \npos_embeddings=pos_embedding_layer[torch.arange(max_length)] # we generate the positional embedding layer\nprint(pos_embeddings.shape) # 4 x 256\n# now we can sum the positional embeddings (4x256) with the token embeddings (8x4x256)\n# broadcasting is done automatically in PyTorch - the new dimension is always added to the left (like Numpy)\ninput_embeddings=token_embeddings+pos_embeddings # 8x4x256\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Simplified self attention","metadata":{}},{"cell_type":"code","source":"inputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nwords=['Your', 'journey', 'starts', 'with', 'one', 'step']\nx_coords=inputs[:,0].numpy()\ny_coords=inputs[:,1].numpy()\nz_coords=inputs[:,2].numpy()\n\n# create 3-D plot\nfig=plt.figure()\nax=fig.add_subplot(111,projection='3d')\n# plot each point and annotate with corresponding word\nfor x,y,y,word in zip(x_coords,y_coords,z_coords,words):\n    ax.scatter(x,yz)\n    ax.text(x,y,z,word,fontsize=10)\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.title('3D Plot of Word Embeddings')\nplt.show()\n\n# calculate the attention score for the second token (as query)\nquery=inputs[1]\nattn_scores_2=torch.empty(inputs.shape[0])\nfor i,x_i in enumerate(inputs):\n    attn_scores_2[i]=torch.dot(x_i,query)\nprint(attn_scores_2)\n\n# L1 normalization \nattn_scores_2_l1=attn_scores_2/attn_scores_2.sum()\n\n# softmax normalization\ndef softmax_naive(x):\n    return torch.exp(x)/torch.exp(x).sum(dim=0)\n    \nattn_scores_2_naive=softmax_naive(attn_scores_2)\n\n# PyTorch implementation of softmax to avoid numeric instability\nattn_scores_2_softmax=torch.softmax(attn_scores_2,dim=0)\n\n# calculate the context vector for the second input\ncontext_vec_2=torch.zeros(query.shape)\nfor i,x_i in enumerate(inputs):\n    context_vec_2+=attn_scores_2_softmax[i]*x_i\n\n# now we can calculate the context vectors for all the inputs\nattn_scores=torch.empty(6,6)\n#for i,x_i in enumerate(inputs):\n#    for j,x_j in enumerate(inputs):\n#        attn_scores[i,j]=torch.dot(x_i,x_j)\nattn_scores=inputs @ inputs.T \nattn_ws=torch.softmax(attn_scores,dim=-1)\nall_context_vectors=attn_ws @ inputs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Self attention with trainable weights","metadata":{}},{"cell_type":"code","source":"x_2=inputs[1]\ndim_in=inputs.shape[1]\ndim_out=2\ntorch.manual_seed(123)\nwq=torch.nn.Parameter(torch.rand(dim_in,dim_out),requires_grad=False)\nwk=torch.nn.Parameter(torch.rand(dim_in,dim_out),requires_grad=False)\nwv=torch.nn.Parameter(torch.rand(dim_in,dim_out),requires_grad=False)\n\n# query key value\nq_2=x_2 @wq\nk_2=x_2 @wk\nv_2=x_2 @wv\n\n# compute attention scores\nqueries=inputs @ wq\nkeys=inputs @ wk\nvalues=inputs @ wv\nkey_2=keys[1]\nattn_score_22=key_2.dot(queries[1]) # example or query 2 with key 2\nattn_scores_2=queries[1]@ keys.T # example for query 2 with all keys\nattn_scores=queries @ keys.T \n# normalize\nattn_ws=torch.softmax(attn_scores/dim_out ** 0.5,dim=-1)\n# attention matrix\nattn=attn_ws @ values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Self attention Python class","metadata":{}},{"cell_type":"code","source":"class SelfAttention_v1(nn.Module):\n    def __init__(self,d_in,d_out):\n        super.__init__()\n        self.W_query=nn.Parameter(torch.rand(d_in,d_out))\n        self.W_key=nn.Parameter(torch.rand(d_in,d_out))\n        self.W_value=nn.Parameter(torch.rand(d_in,d_out))\n\n    def forward(self,x):\n        keys=x @ self.W_key\n        queries=x @ self.W_query\n        values=x @ self.W_value\n        attn_scores=queries @ keys.T\n        attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n        context_vec=attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(123)\nsa_v1=SelfAttention_v1(2,2)\nprint(sa_v1(inputs))\n\n# we can use nn.Linear - uses a more sophistated mechanism to initiate weights\nclass SelfAttention_v2(nn.Module):\n    def __init__(self,d_in,d_out,qkv_bias=False):\n        super.__init__()\n        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n\n    def forward(self,x):\n        keys=self.W_key(x)\n        queries=self.W_query(x)\n        values=self.W_value(x)\n        attn_scores=queries @ keys.T\n        attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n        context_vec=attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(777)\nsa_v2=SelfAttention_v2(2,2)\nprint(sa_v2(inputs))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Masking future inputs in attention weight matrix","metadata":{}},{"cell_type":"code","source":"def mask_attn_ws(attn_ws):\n    context_length=attn_ws.shape[0]\n    mask_simple=torch.tril(torch.ones(context_length,context_length)) # make a lower triangular mask \n    masked_attn=attn_ws*mask_simple\n    # normalize\n    row_sums=marked_attn.sum(dim=1,keepdim=True)\n    masked_attn_norm=masked_attn/row_sums\n    return masked_attn_norm\n\n# better mask \ndef mask_attn_scores_efficient(attn):\n    context_length=attn.shape[0]\n    mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n    masked=attn.masked_fill(mask.bool(),-torch.inf)\n    ws=torch.softmax(masked/keys.shape[-1]**0.5,dim=1)\n    return ws\n\n# dropout\ntorch.manual_seed(123)\ndroput=torch.nn.Dropout(0.5)\nexample=torch.ones(6,6)\nprint(dropout(example))\n \ndef apply_dropout(attn_ws,drop_out_rate=0.5): \n    # it zeros out half of the elemnts and multiplies the one not zeroed out by 2\n    droput=torch.nn.Dropout(drop_out_rate)\n    return dropout(attn_ws)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We want to build a causal attention Python class\nWe also want this class to be able to handle batches with more than one input","metadata":{}},{"cell_type":"code","source":"batch=torch.stack((inputs,inputs),dim=0) # if inputs has size of 6x3, the batch will have a size of 2x6x3\n\nclass CausalAttention(nn.Module):\n    def __init__(self,d_in,d_out,context_length,dropout,qkv_bias=False):\n        super.__init__()\n        self.d_out=d_out\n        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.dropout=dropout\n        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n\n    def forward(self,x): # x has size of batch_size x max_len x dim_embed\n        b,num_tokens,d_in =x.shape\n        keys=self.W_key(x) # broadcast on dim=0\n        queries=self.W_query(x)\n        values=self.W_value(x)\n        attn_scores=queries @ keys.transpose(1,2) # transpose on dim 1 et dim 2 (dim=0 is the batch dim)\n        attn_scores.masked_fill_(\n            self.mask.bool()[:num_tokens,:num_tokens],-torch.inf) # to account for the cases where\n        # the number of tokens < context_size; especially for the ending batch!\n        attn_ws=torch.softmax(attn_scores/key.shape[-1]**0.5,dim=-1)\n        attn_ws=self.dropout(attn_ws)\n        context_vec=attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(123)\ncontext_length=batch.shape[1]\nca=CausalAttention(d_in,d_out,context_length,0.0)\ncontext_vecs=ca(batch)\nprint(context_vec.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note for register_buffer in PyTorch - we use it because it is a static matrix and we are not training the content.\n\nWhen we use causal attention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU). This means that we don't need to manually ensure that these tensors are on the same device as the model parameters, avoiding device mismatch errors.","metadata":{}},{"cell_type":"markdown","source":"Multihead attention","metadata":{}},{"cell_type":"code","source":"class MultiHead(nn.Module):\n    def __init__(self,d_in,d_out,context_len,dropout,num_heads,qkv_bias=False):\n        super().__init__()\n        self.heads=nn.ModuleList([CausalAttention(d_in,d_out,context_len,dropout,qkv_bias) for _ in range(num_heads)])\n    def forward(self,x): # stack the context vectors across the columns\n        return torch.cat([head(x) for head in self.heads],dim=-1)\n\nmha=MultiHead(d_in,d_out,context_length,0.0,num_heads=2)\ncontext_vecs=mha(batch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can combine all the weights matrices together to do just one multiplication instead of many in separate heads.","metadata":{}},{"cell_type":"code","source":"class MultiHeadV2(nn.Module):\n     def __init__(self,d_in,d_out,context_len,dropout,num_heads,qkv_bias=False):\n        super().__init__()\n        assert (d_out % num_heads==0), \"d_out must be a multiple of num_heads\"\n        self.d_out=d_out\n        self.num_heads=d_out//num_heads  # the out dimension for each head\n        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias) # those are concatenated matrices\n        self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.out_proj=nn.Linear(d_out,d_out) # linear layer to combine head outputs\n        self.dropout=dropout\n        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n         \n    def forward(self,x): \n        b,num_tokens,d_in =x.shape\n        keys=self.W_key(x) # broadcast on dim=0\n        queries=self.W_query(x)\n        values=self.W_value(x)\n\n        # we implicitly split the matrix by adding a num_heads dim\n        # unroll last dim: (b, num_tokens, d_out) => (b, num_tokens, num_heads, head_dim)\n        keys=keys.view(b,num_tokens,self.num_heads,self.head_dim)\n        queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n        values=values.view(b,num_tokens,self.num_heads,self.head_dim)\n\n        # transpose to match num_tokens with head_dim to prepare the 4-dim matrix for matmul\n        keys=keys.transpose(1,2)\n        queries=queries.transpose(1,2)\n        values=values.transpose(1,2)\n\n        # compute attn scores by doing dot-products\n        attn_scores=queries @ keys.transpose(2,3) # transpose on dim 1 et dim 2 (dim=0 is the batch dim)\n        attn_scores.masked_fill_(\n            self.mask.bool()[:num_tokens,:num_tokens],-torch.inf) # to account for the cases where\n        # the number of tokens < context_size; especially for the ending batch!\n        attn_ws=torch.softmax(attn_scores/key.shape[-1]**0.5,dim=-1)\n        attn_ws=self.dropout(attn_ws)\n        context_vec=attn_weights @ values # now the shape is (batch, num_heads, num_tokens, head_dim)\n        context_vec=context_vec.transpose(1,2) # we swtich the shape back to (batch, num_tokens, num_heads, head_dim)\n\n        # we want to combine the two last dimensions back to one\n        context_vec=context_vec.contiguous().view(b,num_tokens,self.d_out) # roll up\n        context_vec=slef.out_proj(context_vec) # optional projection\n        return context_vec\n\ntorch.manual_seed(123)\nbatch_size,context_size,d_in =batch.shape\nd_out=6\nmha=MultiHeadV2(d_in,d_out,context_length,0.0,num_heads=2)\ncontext_vecs=mha(batch)\nprint(context_vecs.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Build GPT-2 Architecture","metadata":{}},{"cell_type":"code","source":"GPT_CONFIG_124M={\n    \"vocab_size\":50257,\n    \"context_length\":1024,\n    \"emb_dim\":768,\n    \"n_heads\":12,\n    \"n_layers\":12,\n    \"drop_rate\":0.1,\n    \"qkv_bias\":False\n}\n\nclass DummyGPTModel(nn.Module):\n    def __init__(self,cfg):\n        super.__init__()\n        self.tok_emb=nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"]) # create the vocab embedding of 50257 x 768\n        self.pos_emb=nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"]) # context_length x vocab embedding dim\n        self.drop_emb=nn.Dropout(cfg[\"drop_rate\"])\n        # use a placeholder for TransformerBlock\n        self.trf_blocks=nn.Sequential(\n            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n        )\n        # use a placeholder for LayerNorm\n        self.final_norm=DummyLayerNorm(cfg[\"emb_dim\"])\n        self.out_head=nn.Linear(cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False)\n        \n    def forward(self,in_idx):\n        batch_size,seq_len=in_idx.shape # batch_size x context_len\n        tok_embeds=self.tok_emb(in_idx) # batch_size x context_len x dim_embed\n        # torch.arange places the index for each position and clone the positions for each input in the batch\n        pos_embeds=self.pos_emb(torch.arange(seq_len,device=in_idx.device)) # context_size x dim_embed\n        x=tok_embeds+pos_embeds # broadcast pos_embeds for dim=0, i.e. batch_size\n        x=self.drop_emb(x) # dropout layer\n        x=self.trf_blocks(x) # transformer layer\n        x=self.final_norm(x) # final norm\n        logits=self.out_head(x) # output head, batch_size x context_size x vocab_size\n        return logits\n\n\nclass DummyTransformerBlock(nn.Module):\n    def __init__(self,cfg):\n        super().__init__()\n    def forward(self,x):\n        return x\n\nclass DummyLayerNorm(nn.Module):\n    def __init__(self,normalized_shape,eps=1e-5):\n        super().__init__()\n\n    def forward(self,x):\n        return x\n\n# let's start with input sequences\ntokenizer=tiktoken.get_encoding(\"gpt2\")\nbatch=[]\ntxt1=\"Every effort moves you\"\ntxt2=\"Every day holds a\"\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch=torch.stack(batch,dim=0)\nprint(batch) # gives a tensor of size batch_size x context_size (each element is a tokenId)\n# then we can convert the tokenIds into token embeddings (756-dim vectors)\n\n\ngpt=DummyGPTModel(GPT_CONFIG_124M)\nlogits=gpt(batch)\nprint(logits.shape)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Normalization\n\nWe have batches, multiple neurons","metadata":{}},{"cell_type":"code","source":"# example\ntorch.set_printoptions(sci_mode=False)\ntorch.manual_seed(123)\nbatch_example=torch.randn(2,5)\nlayer=nn.Sequential(nn.Linear(5,6),nn.ReLU())\nout=layer(batch_example) # construct example input\n# now we perform normalization for each batch\nmean=out.mean(dim=-1,keepdim=True) # take mean across the columns, keep dim to maintain dim => batch_size x 1\nvar=out.var(dim=-1,keepdim=True) # same\nout_norm=(out-mean)/torch.sqrt(var) # broacast\nmean_norm=out_norm.mean(dim=-1,keepdim=True)\nvar_norm=out_norm.var(dim=-1,keepdim=True)\n\n# normalization class\nclass NormalizationLayer(nn.Module):\n    def __init__(self,emb_dim):\n        super().__init__()\n        self.eps=1e-5\n        self.scale=nn.Parameter(torch.ones(emb_dim))\n        self.shift=nn.Parameter(torch.zeros(emb_dim))\n    def forward(self,x):\n        mean=x.mean(dim=-1,keepdim=True)\n        var=x.var(dim=-1,keepdim=True,unbiased=False)\n        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n        return self.scale*norm_x+self.shift # scale and shift are trainable\n\nln=NormalizationLayer(embed_dim=5)\nprint(ln(batch_example))\nmean=out_ln.mean(dim=-1,keepdim=True)\nvar=out_ln.var(dim=-1,keepdim=True,unbiased=False)\nprint(mean,var)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"GELU Activation","metadata":{}},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super.__init__()\n    def forward(self,x):\n        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715 *torch.pow(x,3))))\n\ngelu,relu=GELU(),nn.ReLU()\nx=torch.linspace(-3,3,100)\ny_gelu,y_relu=gelu(x),relu(x)\nplt.figure(figsize=(8,3))\nfor i,(y,label) in enumerate(zip([y_gelu,y_relu],[\"GELU\",\"RELU\"]),1):\n    plt.subplot(1,2,i)\n    plt.plot(x,y)\n    plt.title(f\"{label} act func\")\n    plt.xlabel(\"x\")\n    plt.ylabel(f\"{label}(x)\")\n    plt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# test GELU in a NN\nclass FeedForward(nn.Module):\n    def __init__(self,cfg):\n        super.__init__()\n        self.layers=nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]), # expansion\n            GELU(),\n            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"]) # contraction\n        )\n    def forward(self,x):\n        return self.layers(x)\n        \nff=FeedForward(GPT_CONFIG_124M)\nx=troch.rand(2,3,768)\nout=ff(x)\nprint(out.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Skip connections","metadata":{}},{"cell_type":"code","source":"class ExDNN(nn.Module):\n    def __init__(self,layer_sizes,use_shortcut):\n        super.__init__()\n        self.use_shortcut=use_shortcut\n        self.layers=nn.ModleList([\n            nn.Sequential(nn.Linear(layer_sizes[i],layer_sizes[i+1],GELU())) for i in range(len(layer_sizes)-1)\n        ])\n    def forward(self,x):\n        for layer in self.layers:\n            layer_out=layer(x)\n            if self.use_shortcut and x.shape==layer_output.shape:\n                x+=layer_out\n            else:\n                x=layer_out\n        return x\n\nlayer_sizes=[3,3,3,3,3,1]\ninputs=torch.tensor([[1.,0.,-1.]])\ntorch.manuel_seed(123)\nDNN_skip=ExDNN(layer_sizes,True)\nDNN_skip(inputs)\n\ndef print_gradients(model,x):\n    output=model(x)\n    target=torch.tensor([[0.]])\n    #calculate loss based on how close the target and output are\n    loss=nn.MSELoss()\n    loss=loss(output,target)\n    # backward pass to calculate the gradients\n    loss.backward()\n    for name,param in model.named_parameters():\n        if 'weight' in name:\n            print(f\"{name} has grad of {param.grad.abs().mean().item()}\")\n            \nprint_gradients(ExDNN(layer_sizes,False),inputs) # without skips\nprint_gradients(DNN_skip,inputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Coding the whole transformer block","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self,cfg):\n        super.__init__()\n        self.att=MultiHead(\n            d_in=cfg[\"emb_dim\"],d_out=cfg[\"emb_dim\"],context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"],dropout=cfg[\"drop_rate\"],qkv_bias=cfg[\"qkv_bias\"]\n        )\n        self.ff=FeedForward(cfg)\n        self.norm1=NormalizationLayer(cfg[\"emb_dim\"])\n        self.norm2=NormalizationLayer(cfg[\"emb_dim\"])\n        self.drop_shortcut(x)=nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self,x):\n        shortcut=x # shortcut conn for att block\n        x=self.norm1(x)\n        x=self.att(x)\n        x=self.drop_shortcut(x)\n        x+=shortcut\n        shortcut=x # short cut for feedfws\n        x=self.norm2(x)\n        x=self.ff(x)\n        x=self.drop_shortcut(x)\n        x+=shortcut\n        return x\n\ntorch.manual_seed(123)\nx=torch.rand(2,4,768)\ntb=TransformerBlock(GPT_CONFIG_124M)\nprint((tb(x)).shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's build the entire GPT-2","metadata":{}},{"cell_type":"code","source":"class GPT2(nn.Module):\n    def __init__():\n        super.__init__(cfg)\n        self.tok_emb=nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"]) # create the vocab embedding of 50257 x 768\n        self.pos_emb=nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"]) # context_length x vocab embedding dim\n        self.drop_emb=nn.Dropout(cfg[\"drop_rate\"])\n        # use a placeholder for TransformerBlock\n        self.trf_blocks=nn.Sequential(\n            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n        )\n        # use a placeholder for LayerNorm\n        self.final_norm=NormalizationLayer(cfg[\"emb_dim\"])\n        self.out_head=nn.Linear(cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False)\n        \n    def forward(self,in_idx):\n        batch_size,seq_len=in_idx.shape # batch_size x context_len\n        tok_embeds=self.tok_emb(in_idx) # batch_size x context_len x dim_embed\n        # torch.arange places the index for each position and clone the positions for each input in the batch\n        pos_embeds=self.pos_emb(torch.arange(seq_len,device=in_idx.device)) # context_size x dim_embed\n        x=tok_embeds+pos_embeds # broadcast pos_embeds for dim=0, i.e. batch_size\n        x=self.drop_emb(x) # dropout layer\n        x=self.trf_blocks(x) # transformer layer\n        x=self.final_norm(x) # final norm\n        logits=self.out_head(x) # output head, batch_size x context_size x vocab_size\n        return logits\n\n\ngpt2=GPT2(GPT_CONFIG_124M)\nprint((gpt2(x)).shape)\n# how many parameters\ntotal_params=sum(p.numel() for p in gpt2.parameters())\nprint(gpt2.out_head.weight.shape)\ntotal_params2=total_params-sum(p.numel() for p in gpt2.out_head.parameters()) # all the params\ntotal_size_bytes+total_params*4 # each is 4 bytes \ntotal_size_mb=total_size_bytes/1024**2 # 622mb\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now how to use GPT-2 to predict the next token?","metadata":{}},{"cell_type":"code","source":"def generate_text_simple(model,idx,max_new_tokens,context_size): # idx= batch_size x context_size\n    for _ in range(max_new_tokens):\n        idx_cond=idx[:,-context_size:] # get the last tokens\n        with torch.no_grad():\n            logits=model(id_cond)\n        logits=logits[:,-1,:] # last context, batch_size x vocab_size\n        probs=torch.softmax(logits,dim=-1) # batch_size x vocab_size - it is redundant since softmax is monotonous; but we might want also not only the best prediction\n        idx_next=torch.argmax(probs,dim=-1,keepdim=True) # batch_size x 1 (we keep the second dim for concat)\n        idx=torch.cat((idx,idx_next),dim=-1) # batch x (n_tokens + 1)\n    return idx\n \n\nstart_sentence=\"hello i am\"\nencoded=tokenizer.encode(start_sentence)\nencoded_tensor=torch.tensor(encoded).unsqueeze(0) # add an extra dim to the left => batch_size x context_size\nprint(encoded_tensor.shape)\n\ngpt2.eval() # eval mode because we are not training the model, which disables random components like dropout => more efficient\nout=generate_text_simple(model=gpt2,idx=encoded_tensor,max_new_tokens=6,context_size=GPT_CONFIG_124M[\"context_length\"])\nprint(out.shape)\nprint(tokenizer.decode(out.squeeze(0).tolist())) # lose the first dim then tolist for decoding","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we need to train the model. Let's look at loss functions","metadata":{}},{"cell_type":"code","source":"CONFIG={\n    \"vocab_size\":50257,\n    \"context_length\":256,\n    \"emb_dim\":768,\n    \"n_heads\":12,\n    \"n_layers\":12,\n    \"drop_rate\":0.1,\n    \"qkv_bias\":False\n}\n\ndef text_to_token_ids(text,tokenizer):\n    encoded=tokenizer.encode(text,allowe_special={'<|endoftext|>'})\n    encoded_tensor=torch.tensor(encoded).unsqueeze(0) # add batch dim\n    return encoded_tensor\ndef token_ids_to_text(ids,tokenizer):\n    flat=ids.unsqueez(0)\n    return tokenizer.decode(flat.tolist())\n\nstart_context=\"every effort moves you\"\ntokenizer=tiktoken.get_encoding(\"gpt2\")\ntoken_ids=generate_text_simple(model=model,idx=text_to_token_ids(start_context,tokenizer),\n                              max_new_tokens10,context_size=CONFIG[\"context_length\"])\nprint(\"output text: \",token_ids_to_text(token_ids,tokenizer))\n\ninputs=torch.tensor(torch.rand(2,3)) # token ids\ntargets=torch.tensor(torch.rand(2,3)) # token ids\nwith torch.no_grad():\n    logits=gpt2(input) # logits batch_size x context_size x vocab_size\nprobs=torch.softmax(logits,dim=-1)\noutput=torch.argmax(probs,dim=-1,keepdim=True) # batch_size x context_size x 1\nprint(token_ids_totext(targets[0],tokenizer))\nprint(token_ids_to_text(output[0].flatten(),tokenizer))\n\n# cross entropy loss\nbatch_id=0\ntarget_prob=probs[batch_id,[i for i in range(input.shape[1])],targets[batch_id]]\nbatch_id=1\ntarget_prob2=probs[batch_id,[i for i in range(input.shape[1])],targets[batch_id]]\ncross_entropy=-torch.mean(torch.log(torch.cat([target_prob,target_prob2])))\n\n# another way to do cross entropy\nlogits_flat=logits.flatten(0,1) # before: batch_size x context_len x vocab_size; after, (batch_size x context_len) x vocab_size\ntargets_flat=targets.flatten # before: batch_size x context_len; after : (batch_size x context_len)\ntorch.nn.functional.cross_entropy(logits_flat,targets_flat)\n\n# another metric: perplexity\n\n# define the loss fun\ndef calc_loss_batch(input_batch,target_batch,model,device):\n    input_batch,target_batch=input_batch.to(device),target_batch.to(device)\n    logits=model(input_batch)\n    loss=torch.nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())\n    return loss\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's train on real data","metadata":{}},{"cell_type":"code","source":"# we can train the model on raw_data\nprint(len(raw_data),len(tokenizer.encode(raw_data)))\n# divide data into training and validation - we need context_size and stride\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self,txt,tokenizer,max_len,stride):\n        self.input_ids=[]\n        self.target_ids=[]\n        token_ids=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n        for i in range(0,len(token_ids)-max_len,stride):\n            input_chunk=token_ids[i:i+max_len]\n            target_chunk=token_ids[i+1:i+max_len+1]\n            self.input_ids.append(input_chunk)\n            self.target_ids.append(target_chunk)\n\n    def __len__(self):\n        return len(self.input_ids)\n    def __getitem__(self,idx):\n        return self.input_ids[idx],self.target_ids[idx]\n\ndef create_data_loader(txt,batch_size=4,max_len=256,stride=128,shuffle=True,drop_last=True,\n                      num_worker=0):\n    tokenizer=tiktoken.get_encoding(\"gpt2\")\n    dataset=GPTDatasetV1(txt,tokenizer,max_len,stride)\n    dataloader=DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,\n                         num_workers=num_workers)\n    return dataloader\n\ntrain_ratio=0.9\nsplit_idx=int(train_ratio*len(raw_data))\ntrain_data=raw_data[:split_idx]\nval_data=raw_data[split_idx:]\ntorch.manual_seed(123)\ntrain_loader=create_dataloader_v1(train_data,2,CONFIG[\"context_length\"],CONFIG[\"context_length\"],\n                               True,True,0)\nval_loader=create_dataloader_v1(val_data,2,CONFIG[\"context_length\"],CONFIG[\"context_length\"],\n                               True,True,0)\n\nfor x,y in train_loader:\n    print(x.shape,y.shape) # will iterate through all the batches, each batch having batch_size x context_len\nprint(len(train_loader),len(val_loader))\ntrain_tokens=0,val_tokens=0\nfor input_batch,target_batch in train_loader:\n    train_tokens+=input_batch.numel()\nfor input_batch,target_batch in val_loader:\n    val_tokens+=input_batch.numel()\nprint(train_tokens,val_tokens)\n\n# calculate losses w dataloader\ndef calc_loss_loader(data_loader,model,device,num_batches=None):\n    total_loss=0\n    if len(data_loader)==0:\n        return float(\"nan\")\n    elif num_batches==None:\n        num_batches=len(data_loader)\n    else:\n        for i, (input_batch,target_batch) in data_loader:\n            if i<num_batches:\n                loss=calc_loss_batch(input_batch,target_batch,model,device)\n                total_loss+=loss\n            else:\n                break\n        return total_loss/num_batches # the point of having batches is to avg all the losses\n\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ntorch.manual_seed(123)\nwith torch.no_grad():\n    train_loss=calc_loss_loader(train_loader,gpt2,device)\n    val_loss=calc_loss_loader(val_loader,gpt2,device)\n\nprint(train_loss,val_loss)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"How to train? Back propagation","metadata":{}},{"cell_type":"code","source":"def train_model_simple(model,train_loader,val_loader,optimizer,device,\n                      num_epochs,eval_freq,eval_iter,start_context,tokenizer):\n    train_losses,val_losses,track_tokens_seen=[].[],[]\n    tokens_seen,global_step=0,-1\n    # main training loop\n    for epoch in range(num_epochs):\n        model.train() # set model to training mode\n        for input_batch,target_batch in train_loader:\n            optimizer.zero_grad() # reset loss gradients from previous batch iteration\n            loss=calc_loss_batch(input_batch,target_batch,model,device)\n            loss.backward() # calculate loss gradients\n            optimizer.step() # update model weights using loss gradients\n            tokens_seen+=input_batch.numel()\n            global_step+=1\n            # optional eval step\n            if global_step % eval_freq==0:\n                train_loss,val_loss=evaluate_model(model,train_loader,val_loader,device,eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"ep {epoch+1} (step {global_step:06d}\")\n        generate_and_print_sample(model,tokenizer,device,start_context)\n    return train_losses,val_losses,track_tokens_seen\n\ndef evaluate_model(model,train_loader,val_loader,device,eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss=calc_loss_loader(train_loader,model,device,num_batches=eval_iter)\n        val_loss=calc_loss_loader(val_loader,model,device,num_batches=eval_iter)\n    model.train()\n    return train_loss,val_loss\n\ndef generate_and_print_sample(model,tokenizer,device,start_context):\n    model.eval()\n    context_size=model.pos_emb.weight.shape[0]\n    encoded=text_to_token_ids(start_contxt,tokenizer).to(device)\n    with torch.no_grad():\n        token_ids=generate_text_simple(model=model,idx=encoded,max_new_tokens=50,context_size=context_size)\n    decoded_text=token_ids_to_text(token_ids,tokenizer)\n    print(decoded_text.replace(\"\\n\",\" \"))\n    model.train()\n\nimport time\nstart_time=time.time()\ntorch.manual_seed(123)\nmodel=GPT2(CONFIG)\nmodel.to(device)\noptimizer=torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay=0.1)\nnum_epochs=10\ntrain_losses,val_losses,tokens_seen=train_model_simple(\n    model,train_loader,val_loader,optimizer,device,num_epochs=num_epochs,eval_freq=5,eval_iter=5\n    ,start_context=\"Every effort moves you\",tokenizer=tokenizer\n)\nend_time=time.time()\nexecution_time_minutes=(end_time-start_time)/60\nprint(execution_time_minutes,\" mins\")\n\nfrom matplotlib.ticker import MaxNLocator\ndef plot_losses(epochs_seen,tokens_seen,train_losses,val_losses):\n    fig,ax1=plt.subplots(figsize=(5,3))\n    ax1.plot(epochs_seen,train_losses,label=\"Training loss\")\n    ax1.plot(epochs_seen,val_losses,linestyle=\"-.\",label=\"Val loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2=ax1.twiny()# create a second x-axis that shares the same y-axis\n    ax2.plot(tokens_seen,train_losses,alpha=0)\n    ax2.set_xlabel(\"Tokens seen\")\n    fig.tight_layout()\n    plt.savefig(\"loss-plot.pdf\")\n    plt.show()\nepochs_tensor=torch.linespace(0,num_epochs,len(train_losses))\nplot_losses(epochs_tensor,tokens_seen,train_losses,val_losses)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"techniques to control randomness\n\n1. temperature scaling\n2. top-k sampling","metadata":{}},{"cell_type":"code","source":"model.to(\"cpu\")\nmodel.eval()\n\ntoken_ids=generate_text_simple(model=model,idx=text_to_token_ids(\"every effort moves you\"),\n                              tokenizer,max_new_tokens=25,context_size=CONFIG[\"context_length\"])\nprint(token_ids_to_text(token_ids))\n# very random!\n\n# strategy 1: temperature scaling\nvocab = { \n    \"closer\": 0,\n    \"every\": 1, \n    \"effort\": 2, \n    \"forward\": 3,\n    \"inches\": 4,\n    \"moves\": 5, \n    \"pizza\": 6,\n    \"toward\": 7,\n    \"you\": 8,\n} \n\ninverse_vocab = {v: k for k, v in vocab.items()}\nnext_token_logits = torch.tensor(\n[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\nnext_token_logits2 = next_token_logits/0.1\nnext_token_logits3 = next_token_logits/5\nprobas = torch.softmax(next_token_logits2, dim=0)\nprint(probas)\nprobas = torch.softmax(next_token_logits3, dim=0)\nprint(probas)\nprobas = torch.softmax(next_token_logits, dim=0)\nprint(probas)\nnext_token_id = torch.argmax(probas).item()\nprint(next_token_id)\nprint(inverse_vocab[next_token_id])\n\ndef print_sampled_tokens(probas):\n    torch.manual_seed(123)\n    sample=[torch.multinomial(probas,num_samples=1).item()] # to simulate sampling\n    sampled_ids=torch.bincount(torch.tensor(sample))\n    for i,freq in enumerate(sampled_ids):\n        print(f\"{freq} x {inverse_vocab[i]}\")\nprint_sampled_tokens(probas)\n\ndef softmax_with_t(logits,temp):\n    scaled_logits=logits/temp\n    return torch.softmax(scaled_logits,dim=0)\ntemps=[0,0.1,5]\nscaled_probas={softmax_with_t(next_token_logits,T) for T in temps}\nx=torch.arange(len(vocab))\nbar_width=0.15\nfig,ax=plt.subplot(figsize=(5,3))\nfor i,T in enumerate(temps):\n    rects=ax.bar(x+i*bar_width,scaled_probas[i],bar_width,label=f'temp={T}')\nax.set_ylabel('prob')\nax.set_xticks(x)\nax.set_xticklabels(vocab.keys(),rotation=90)\nax.legend()\nplt.tight_layout()\nplt.savefig(\"temp-plot.pdf\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Top-k Sampling","metadata":{}},{"cell_type":"code","source":"top_k=3\ntop_logits,top_pos=torch.topk(next_token_logtis,top_k)\nnew_logits=torch.where(condition=next_token_logits<top_logits[-1],input=torch.tensor(float(\"-inf\")),\n                      other=next_token_logits) # replace the low values by -inf\nprobs=torch.softmax(new_logits)\n\ndef generate(model,idx,max_new_tokens,context_size,temperature=0.0,top_k=None,eos_id=None):\n    for _ in range(max_new_tokens):\n        idx_cond=idx[:,-context_size:]\n        with torch.no_grad():\n            logits=model(idx_cond) # batch_size x context_size x vocab_size\n        logits=logits[:,-1,:] # only the last step for the next word pred for the whole seq\n        # top-k\n        if top-k is not None:\n            top_logits,_=torch.topk(logits,top_k)\n            min_val=top_logits[:,-1] # batch_size x 1\n            logits=torch.where(logits<min_val,torch.tensor(float(\"-inf\")).to(logits.device),\n                              logits)\n        if temperature>0.0:\n            logits=logits/temperature\n            probs=torch.softmax(logits,dim=-1)\n            idx_next=torch.multinomial(probs,num_samples=1)\n        else:\n            idx_next=torch.argmax(logits,dim=-1,keepdim=true)\n        if idx_next==eos_id: # if end of seq token is hit\n            break # we end the loop earlier\n        idx=torch.cat((idx,idx_next),dim=1) # concat on the dim of context_size\n    return idx\n\ntoken_ids_with_top_k_and_temp=generate(model,text_to_token_ids(\"every effort loves you\",tokenizer),\n                                      max_new_tokens=15,context_size=CONFIG[\"context_length\"],\n                                      top_k=25,temperature=1.4)\nprint(token_ids_to_text(token_ids_with_top_k_and_temp))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Save and load the params using PyTorch","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(),\"model.pth\")\nmodel.load_state_dict(torch.load(\"model.pth\"))\n\n# we can save the optimizer too\noptimizer=torch.optim.AdamW(model.parameters,lr=0.0004,weights_decay=0.1)\ntorch.save({\n    \"model_state_dict\":model.state_dict(),\n    \"optimizer_state_dict\":optimizer.state_dict(),\n}, \"model_and_optim.pth\")\n\ncheckpoint=torch.load(model_and_optim.pth)\nmodel=GPT2(CONFIG)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer=torch.optim.AdamW(model.parametesr(),lr=5e-4,weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\nmodel.train() # training mode","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can load the pretrained GPT-2 weights from OpenAi\n\nThe weights are stocked in TensorFlow => need to download it; we can use tqdm to track the download","metadata":{}},{"cell_type":"code","source":"pip install tensorflow>=2.15.0 tqdm>=4.66","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tqdm\nprint(tf.__version__,tqdm.__version__)\n\nfrom gpt_download3 import download_and_load_gpt2\n# start the download - select model size and specify rep\nsettings,params=download_and_load_gpt2(model=model_size=\"124M\",models_dir=\"gpt2\")\nprint(settings,params.keys) # params is a dict, settings is the config\nprint(params[\"wte\"].shape()) # check the shape of token embedding weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define model configurations in a dictionary for compactness\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Copy the base configuration and update with specific model settings\nmodel_name = \"gpt2-small (124M)\"  # Example model name\nNEW_CONFIG = GPT_CONFIG_124M.copy()\nNEW_CONFIG.update(model_configs[model_name])\nNEW_CONFIG.update({\"context_length\":1024,\"qkv_bias\":True})\ngpt=GPT2(NEW_CONFIG)\ngpt.eval()\n\n# how to integrate the weights??\n# manually map the weights\ndef load_weights_into_gpt(gpt,params):\n    gpt.pos_emb.weight=assign(gpt.pos_emb.weight,params['wpe']) # we use assign to avoid breaking links\n    gpt.tok_emb.weight=assign(gpt.tok_emb.weight,params['wte'])\n    for b in range(len(params[\"blocks\"])):\n        q_w,k_w,v_w=np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"],3,axis=-1)\n        gpt.trf_blocks[b].att.W_query.weight=assign(gpt.trf_blocks[b].att.W_query.weight,q_w.T)\n        gpt.trf_blocks[b].att.W_key.weight=assign(gpt.trf_blocks[b].att.W_key.weight,k_w.T)\n        gpt.trf_blocks[b].att.W_value.weight=assign(gpt.trf_blocks[b].att.W_value.weight,v_w.T)\n        q_b,k_b,v_b=np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"],3,axis=-1)\n        gpt.trf_blocks[b].att.W_query.bias=assign(gpt.trf_blocks[b].att.W_query.bias,q_b)\n        gpt.trf_blocks[b].att.W_key.bias=assign(gpt.trf_blocks[b].att.W_key.bias,k_b)\n        gpt.trf_blocks[b].att.W_value.bias=assign(gpt.trf_blocks[b].att.W_value.bias,v_b)\n\n        gpt.trf_blocks[b].att.out_proj.weight=assign(gpt.trf_blocks[b].att.out_proj.weight,params[\"blocks\"][b][\"attn\"][\"c_proj\"])[\"w\"].T)\n        gpt.trf_blocks[b].att.out_proj.bias=assign(gpt.trf_blocks[b].att.out_proj.bias,params[\"blocks\"][b][\"attn\"][\"c_proj\"])[\"b\"])\n\n        gpt.trf_blocks[b].ff.layers[0].weight=assign(gpt.trf_blocks[b].ff.layers[0].weight,params[\"blocks\"][b][\"mlp\"][\"c_fc\"])[\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[0].bias=assign(gpt.trf_blocks[b].ff.layers[0].bias,params[\"blocks\"][b][\"mlp\"][\"c_fc\"])[\"b\"])\n        gpt.trf_blocks[b].ff.layers[1].weight=assign(gpt.trf_blocks[b].ff.layers[1].weight,params[\"blocks\"][b][\"mlp\"][\"c_proj\"])[\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[1].bias=assign(gpt.trf_blocks[b].ff.layers[1].bias,params[\"blocks\"][b][\"mlp\"][\"c_proj\"])[\"b\"])\n\n        gpt.trf_blocks[b].norm1.scale=assign(gpt.trf_blocks[b].norm1.scale,params[\"blocks\"][b][\"ln_1\"])[\"g\"])\n        gpt.trf_blocks[b].norm1.shift=assign(gpt.trf_blocks[b].norm1.shift,params[\"blocks\"][b][\"ln_1\"])[\"b\"])\n        gpt.trf_blocks[b].norm2.scale=assign(gpt.trf_blocks[b].norm2.scale,params[\"blocks\"][b][\"ln_2\"])[\"g\"])\n        gpt.trf_blocks[b].norm2.shift=assign(gpt.trf_blocks[b].norm2.shift,params[\"blocks\"][b][\"ln_2\"])[\"b\"])\n\n    gpt.final_norm.scale=assign(gpt.final_norm.scale,params[\"g\"])\n    gpt.final_norm.shift=assign(gpt.final_norm.scale,params[\"b\"])\n    gpt.out_head.weight=assign(gpt.out_head.weight,parms[\"wte\"])\n\nload_weights_into_gpt(gpt,params)\ngpt.to(device)\n\ntorch.manual_seed(123)\ntoken_ids=generate(model=gpt,idx=text_to_token_ids(\"everything effort moves you\",tokenizer).to(device),\n                  max_new_tokens=25,context_size=NEW_CONFIG[\"context_length\"],top_k=50,temperature=1.5)\nprint(\"output\",token_ids_to_text(token_ids,tokenizer))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finetuning - preparing the dataset","metadata":{}},{"cell_type":"code","source":"import urllib.request\nimport ssl\nimport zipfile\nimport os\nfrom pathlib import Path\n\nurl=\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\nzip_path=\"sms_spam_collection.zip\"\nextracted_path=\"sms_spam_collection\"\ndata_file_path=Path(extracted_path) /\"SMSSpamCollection.tsv\"\n\ndef download_and_unzip_spam_data(url,zip_path,extracted_path,data_file_path):\n    if data_file_path.exists():\n        print(\"f{data_file_path} already exists. Skipping download and extraction.\")\n        return\n    # create a unverfied ssl context\n    ssl_context=ssl._create_univerified_context()\n    # downloading the file\n    with urllib.request.urlopen(url,context=ssl_context) as response:\n        with open(zip_path,\"wb\") as out_file:\n            out_file.write(response.read())\n    # unzipping the file\n    with zipfile.ZipFile(zip_path,\"r\") as zip_ref:\n        zip_ref.extractall(extracted_path)\n    # add .tsv file extension\n    original_file_path=Path(extracted_path)/\"SMSSpamCollection\"\n    os.rename(original_file_path,data_file_path)\n    print(f\"file downloaded and saved as {data_file_path}\")\n\ndownload_and_unzip_spam_data(url,zip_path,extracted_path,data_file_path)\n\n\nimport pandas as pd\ndf=pd.read_csv(data_file_path,sep=\"\\t\",header=None,names=[\"Label\",\"Text\"])\nprint(df[\"Label\"].value_counts())\n\ndef create_balanced_dataset(df):\n    num_spam=df[df[\"Label\"]==\"spam\"].shape(0) # spam has less\n    ham_subset=df[df[\"Label\"]==\"ham\"].sample(num_spam,random_state=123)\n    balanced_df=pd.concat([ham_subet,df[df[\"Label\"]==\"spam\"]])\n    return balanced_df\nprint(balanced_df[\"Label\"].value_counts())    \n\nbalanced_df[\"Label\"]=balanced_df[\"Label\"].map({\"ham\":0,\"spam\":1}) # ressembles token ids\n\n# we can split the data into 70% for training, 10% for validation and 20% for testing\ndef random_split(df,tran_frac,val_frac):\n    df=df.sample(frac=1,random_state=123).reset_index(drop=True)\n    train_end=int(len(df)*train_frac)\n    val_end=train_end+int(len(df)*val_frac)\n    train_df=df[:train_end]\n    val_df=df[train_end:val_end]\n    test_df=df[val_end:]\n    return train_df,val_df,test_df\n\ntrain_df,val_df,test_df=random_split(balanced_df,0.7,0.1)\n\nprint(len(train_df),len(val_df),len(test_df))\ntrain_df.to_csv(\"train.csv\",index=None)\nval_df.to_csv(\"val.csv\",index=None)\ntest_df.to_csv(\"test.csv\",index=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dataloader for classification finetuning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass SpamDataset(Dataset):\n    def __init__(self,csv_file,tokenizer,max_length=None,pad_token_id=50256):\n        self.data=pd.read_csv(csv_file)\n        self.encoded_texts=[\n            tokenizer.encode(text) for text in self.data[\"Text\"]\n        ]\n        if max_length is None:\n            self.max_length=self._longest_encoded_length()\n        else:\n            self.max_length=max_length\n            # truncate texts if they are longer than max_length\n            self.encoded_texts={\n                encoded_text[:self.max_length]\n                for encoded_text in self.encoded_texts\n            }\n        # padding\n        self.encoded_texts=[encoded_text+[pad_token_id]*(self.max_length-len(encoded_text))\n                           for encoded_text in self.encoded_texts]\n    def getitem(self,index):\n        encoded=self.encoded_texts[index]\n        label=self.encoded_texts[index][\"Label\"]\n        return (\n            torch.tensor(encoded,dtype=torch.long),\n            torch.tensor(label,dtype=torch.long)\n        )\n    def __len__(self):\n        return len(self.data)\n    def _longest_encoded_length(self):\n        max_length=0\n        for encoded_text in self.encoded_texts:\n            encoded_length=len(encoded_text)\n            if max_length<encoded_length:\n                max_length=encoded_length\n        return max_length\n        \ntrain_dataset=SpamDataset(csv_file=\"train.csv\",max_length=None,tokenizer)\nprint(train_dataset.max_length)\nval_dataset=SpamDataset(csv_file=\"train.csv\",max_length=train_dataset.max_length,tokenizer)\ntest_dataset=SpamDataset(csv_file=\"train.csv\",max_length=train_dataset.max_length,tokenizer)\n\nfrom torch.utils.data import DataLoader\nnum_workers=0\nbatch_size=8\ntorch.manual_seed(123)\ntrain_loader=DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,drop_last=True,\n)\nval_loader=DataLoader(\n    dataset=val_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,drop_last=True,\n)\ntest_loader=DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,drop_last=True,\n)\n\nfor input_batch,target_batch in train_loader:\n    pass\nprint(input_batch.shape,target_batch.shape) # x x y\n# each batch contains x training examples with y tokens each\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Architecture for LLM for Classification Fine-tuning","metadata":{}},{"cell_type":"code","source":"assert train_dataset.max_length<=BASE_CONFIG[\"context_length\"],(\n    \"dataset length exceeds the model's context length. reinit datasets with max_length=context_length\"\n)\n# we have loaded the params into the architecture gpt-2 before\nmodel.eval()\n# now we need to add the classification head to the original architecture\nprint(model) # print the orig architecture\nfor param in model.parameters():\n    param.require_grad=False # freeze all the params\ntorch.manual_seed(123)\nnum_classes=2\nmodel.out_head=torch.nn.Linear(in_features=CONFIG[\"emb_dim\"],out_features=num_classes) # require_grad is True by default\n# unfreeze the parameters that we want to train!\nfor param in model.trf_blocks[-1].parameters():\n    param.require_grad=True\nfor param in model.final_norm.parameters():\n    param.require_grad=True   \n\ninputs=tokenizer.encode(\"do you have time\")\ninputs=torch.tensor(inputs).unsqueeze(0) # add batch dim\nwith torch.no_grad():\n    outputs=model(input) # output is batch_size x context_len x # of classes\n\ndef calc_accuracy_loader(data_loader,model,device,num_batches=None):\n    model.eval()\n    correct_predictions,num_examples=0,0\n    if num_batches is None:\n        num_batches=len(data_loader)\n    else:\n        num_batches=min(num_batches,len(data_loader))\n    for i,(input_batch,target_batch) in enumerate(data_loader):\n        if i<num_batches:\n            input_batch,target_batch=input_batch.to(device),target_batch.to(device)\n            with torch.no_grad():\n                logits=model(input_batch)[:,-1,:] # of the last context => get output token\n            predicted_labels=torch.argmax(logits,dim=-1)\n            num_examples+=predicted_labels.shape[0]\n            correct_preds+=(predicted_labels==target_batch).sum().item()\n        else:\n            break\n    \n    return correct_predictions/num_examples\n\ndevice=torch.device(\"cuda\" if torch.duda.is_available() else \"cpu\")\nmodel.to(device)\ntorch.manual_seed(123)\ntrain_acc=calc_accuracy_loader(train_loader,model,device,num_batches=10)\nval_acc=calc_accuracy_loader(val_loader,model,device,num_batches=10)\ntest_acc=calc_accuracy_loader(test_loader,model,device,num_batches=10)\nprint(train_acc,test_acc,val_acc)\n\n# loss: cross entropy\ndef calc_loss_batch(input_batch,target_batch,model,device):\n    input_batch,target_batch=input_batch.to(device),target_batch.to(device)\n    logits=model(input_batch)[:,-1,:]\n    return torch.nn.functional.cross_entropy(logits,target_batch)\n\ndef calc_loss_loader(data_loader,model,device,num_batches=None):\n    total_loss=0.\n    if len(data_loader)==0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches=len(data_loader)\n    else:\n        num_batches=min(num_batches,len(data_loader))\n    for i,(input_batch,target_batch) in enumerate(dataloader):\n        if i<num_batches:\n            loss=calc_loss_batch(input_batch,target_batch,model,device)\n            total_loss+=loss.item()\n        else:\n            break\n    return total_loss/num_batches\n\nwith torch.no_grad():\n    train_loss=calc_loss_loader(train_loader,model,device,num_batches=5)\nprint(train_loss)\n\ndef train_classifier_simple(model,train_loader,val_loader,optimizer,device,num_epochs,\n                           eval_freq,eval_iter):\n    train_losses,val_losses,train_accs,val_accs=[],[],[],[]\n    examples_seen,global_step=0,-1\n    for epoch in range(num_epochs):\n        model.train()\n        for input_batch,target_batch in train_loader:\n            optimizer.zero_grad()\n            loss=calc_loss_batch(input_batch,target_batch,model_device)\n            loss.backward()\n            optimizer.step()\n            examples_seen+=input_batch.shape[0]\n            global_step+=1\n            if global_step% eval_freq==0:\n                train_loss,val_loss=evaluate_model(model,train_loader,val_loader,device,eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                print(f\"epoch {epoch+1} step ({global_step}) train loss {train_loss:.3f} val loss {val_loss:.3f}\")\n        train_acc=calc_accuracy_loader(train_loader,model,device,num_batches=eval_iter)\n        val_acc=calc_accuracy_loader(val_loader,model,device,num_batches=eval_iter)\n        print(train_acc,val_acc)\n        train_accs.append(train_acc)\n        val_accs.append(val_acc)\n    return train_losses,val_losses,train_accs,val_accs,examples_seen\n\n# then we can plot\ndef evaluate_model(model,train_loader,val_loader,device,eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss=calc_loss_loader(train_loader,model,device,num_batches=eval_iter)\n        val_loss=calc_loss_loader(val_loader,model,device,num_batches=eval_iter)\n    model.train()\n    return train_loss,val_loss\n\nimport time\nstart_time=time.time()\ntorch.manual_seed(123)\noptimizer=torch.optim.AdamW(model.parameters(),lr=1e-5,weight_decay=0.1)\nnum_epochs5\ntrain_losses,val_losses,train_accs,val_accs,exs_seen=train_classifier_simple(\n    model,train_loader,val_loader,optimizer,device,num_epochs=num_epochs,eval_freq=50,eval_iter=5\n)\nend_time=time.time()\nexecution_time_min=(end_time-start_time)/60\nprint(execution_time_min)\n\nepochs_tensor=torch.linespace(0,num_epochs,len(train_losses))\nexs_seen_tensor=torch.linspace(0,exs_seen,len(train_losses))\nplot_values(epochs_tensor,exs_seen_tensor,train_losses,val_losses)\n\n\nplot_values(epochs_tensor,exs_seen_tensor,train_acc,val_accs)\ntrain_acc=calc_accuracy_loader(train_loader,model,device,num_batches=10)\nval_acc=calc_accuracy_loader(val_loader,model,device,num_batches=10)\ntest_acc=calc_accuracy_loader(test_loader,model,device,num_batches=10)\nprint(train_acc,test_acc,val_acc)\n\n# testing model on new data\ndef classify_review(text,model,tokenizer,device,max_length,pad_token_id=50256):\n    model.eval()\n    inputs_ids=tokenizer.encode(text)\n    supported_context_length=model.pos_emb.weight.shape[0]\n    inputs_ids=input_ids[:min(max_length,supported_context_length)]\n    input_ids+[pad_token_id] *(max_length-len(input_ids))\n    input_tensor=torch.tensor(input_ids,device=device).unsqueeze(0)\n    with torch.no_grad():\n        logtis=model(input_tensor)[:,-1,:]\n    predicted_label=torch.argmax(logits,dim=-1).item()\n    return \"spam\" if predicted_label==1 else \"ham\"\n\ntext1=(\"you \",\"kkk\")\nprint(classify_review(text_1,model,tokenizer,device,max_length=train_dataset.max_length))\n\n# save load\ntorch.save(model.state_dict(),\"review_classifier.pth\")\nmodel_state_dict=torch.load(\"review_classifier.pth\")\nmodel.load_state_dict(model_state_dict) # to match the keys!\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Instruction Finetuning\n\n","metadata":{}},{"cell_type":"code","source":"# download the dataset\nimport json,os,urllib,ssl\n\ndef download_and_load_file(file_path,url):\n    ssl_context=ssl.create_default_context()\n    ssl_context.check_hostname=False\n    ssl_context.verify_mode=ssl.CERT_NONE\n    if not os.path.exists(file_path):\n        with urllib.request.urlopen(url,context=ssl_context) as response:\n            text_data=response.read().decode(\"utf-8\")\n        with open(file_path,\"w\",encoding=\"utf-8\") as file:\n            file.write(text_data)\n    else:\n        with open(file_path,\"r\",encoding=\"utf-8\") as file:\n            text_data=file.read()\n    with open(file_path,\"r\",encoding=\"utf-8\") as file:\n        data=json.load(file)\n    return data\n\nfile_path=\"ins-data.json\"\nurl=(\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n    \"/main/ch07/01_main-chapter-code/instruction-json.json\"\n)\ndata=download_and_load_file(file_path,url)\nprint(len(data),data[50])\n\ndef format_input(entry):\n    instruction_text=(\n        f\"Below is an instruction that describes a task.\"\n        f\"Write a response that appropriately complete the request.\"\n        f\"\\n\\n### Instruction:\\n {entry[\"instruction\"]}\")\n    input_text=f\"\\n\\n### Input:\\n{entry[\"input\"]}\" if entry[\"input\"] else \"\"\n    return instruction_text+input_text\n\nprint(format_input(data[50])) # test it out\n\n# split the dataset into training, val and test\ntrain_idx=int(len(data)*0.85)\ntest_idx=int(len(data)*0.1)\nval_idx=len(data)-train_idx-test_idx\ntrain_data=data[:train_idx]\ntest_data=data[train_idx:train_idx+test_idx]\nval_data=data[train_idx+test_idx:]\nprint(len(train_data),len(val_data),len(test_data))\n\n# add responses, tokenize and organize the data into batches\nclass InstructionDataset(Dataset):\n    def __init__(self,data,tokenizer):\n        self.data=data\n        self.encoded_text=[ ] # pretokenize texts\n        for entry in data:\n            instruction_plus_input=format_input(entry)\n            response_text=f\"\\n\\n### Response:\\n{entry['output']}\"\n            full_text=instruction_plus_input+response_text\n            self.encoded_texts.append(tokenizer.encode(full_text))\n    def __getitem(self,i):\n        return self.encoded_texts[i]\n    def __len__(self):\n        return len(self.data)\n\n\n# padding, generate input and target pairs, replace the padding tokens with -100\ndef custom_collate(batch,pad_token_id=50256,device=\"cpu\",ignore_index=-100,allowed_max_len=None):\n    # find the longest seq in the batch, and increase the length by +1 which will add one extra\n    # padding tokens below\n    batch_max_len=max(len(item)+1 for item in batch)\n    input_lst=[]\n    output_lst=[]\n    for item in batch:\n        new_item=item.copy()\n        new_item+=[pad_token_id]\n        padded=(new_item+[pad_token_id]*(batch_max_len-len(new_item)))\n        inputs=torch.tensor(padded[:-1])\n        outputs=torch.tensor(padded[1:])\n        # replace all but the first padding tokens in targets by ignore_index\n        mask=outputs==pad_token_id\n        indices=torch.nonzero(mask).squeeze # get all the padding token positions\n        if indices.numel()>1:\n            outputs[indices[1:]]=ignore_index\n        if allowed_max_len is not None:\n            inputs=inputs[:allowed_max_len]\n            outputs=outputs[:allowed_max_len]\n        input_lst.append(inputs)\n        output_lst.append(outputs)\n    inputs_tensor=torch.stacked(inputs_lst).to(device)\n    outputs_tensor=torch.stacked(outputs_lst).to(device)\n    return inputs_tensor,outputs_tensor # inputs and targets\n\n# test\ni1=[1,2,3,4,5,6]\ni2=[1,3]\ni3=[3,4,5,6]\nbatch=(i1,i2,i3)\nprint(custom_collate(batch))\n\nlogits_1=torch.tensor([[-1.0,1.0],[-0.5,1.5],[-0.5,1.5]])\ntargets_1=torch.tensor([0,1,-100])\nloss=torch.nn.functional.cross_entropy(logits_1,targets_1) # ignore_index=-100 is ignored\nprint(loss)\n\n# create dataloader\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfrom functools import partial \n# new version of the function with the device argument pre-filled\ncustomized_collate_fn=partial(custom_collate,device=device,allowed_max_length=1024)\n\nfrom torch.utils.data import DataLoader\n\nnum_workers=0\nbatch_size=8\ntorch.manual_seed(123)\ntrain_dataset=InstructionDataset(train_data,tokenizer)\nval_dataset=InstructionDataset(val_data,tokenizer)\ntest_dataset=InstructionDataset(test_data,tokenizer)\ntrain_loader=DataLoader(train_data_set,batch_size=batch_size,collate_fn=customized_collate,\n                       shuffle=True,drop_last=True,num_workers=num_workers)\nval_loader=DataLoader(val_data_set,batch_size=batch_size,collate_fn=customized_collate,\n                       shuffle=True,drop_last=True,num_workers=num_workers)\ntest_loader=DataLoader(test_data_set,batch_size=batch_size,collate_fn=customized_collate,\n                       shuffle=True,drop_last=True,num_workers=num_workers)\n\nfor inputs,targets in val_loader:\n    print(inputs.shape,targets.shape) # both batch_size x token_num => max_length for that batch\n# however, between batches, the token_num can differ\n\n# load weights into gpt2 architecture - we have done it previously; likely, we can do it again\nmo=GPT2(CONFIG)\nload_weights_into_gpt(mo,params)\nmo.eval()\ntorch.manual_seed(123)\ninput_t=val_data[0]\ntoken_ids=generate(model=model,idx=text_to_token_ids(input_t,tokenizer),max_new_tokens=35,\n                  context_size=CONFIGT[\"context_length\"],eos_id=50256)\nprint(input_t,token_ids_to_text(token_ids,tokenizer)[len(input_t:)].strip())\n# the pretrained weights are not doing a good job at all!\n\n# training loop - we can use calc_loss_batch, calc_loss_loader defined before\ndef train_instructions(model,train_loader,val_loader,optimizer,device,num_epochs,\n                           eval_freq,eval_iter,start_context=format_input(val_data[0]),tokenizer=tokenizer):\n    train_losses,val_losses=[],[]\n    examples_seen,global_step=0,-1\n    for epoch in range(num_epochs):\n        model.train()\n        for input_batch,target_batch in train_loader:\n            optimizer.zero_grad()\n            loss=calc_loss_batch(input_batch,target_batch,model,device)\n            loss.backward()\n            optimizer.step()\n            examples_seen+=input_batch.shape[0]\n            global_step+=1\n            if global_step% eval_freq==0:\n                train_loss,val_loss=evaluate_model(model,train_loader,val_loader,device,eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                print(f\"epoch {epoch+1} step ({global_step}) train loss {train_loss:.3f} val loss {val_loss:.3f}\")\n        generate_and_print_sample(model,tokenizer,device,start_context)\n    return train_losses,val_losses\n\nstart_time=time.time()\ntorch.manual_seed(123)\noptimizer=torch.optim.AdamW(mo.parameters(),lr=0.00005,weight_decay=0.1)\nnum_epochs=1\ntrain_losses,val_losses,tokens_seen=train_instructions(mo,train_loader,val_loader,optimizer,\n                                                      device,num_epochs,eval_freq=5,5,\n                                                      format_input(val_data[0]),tokenizer)\nend_time=time.time()\nprint(end_time-start_time)\n\n# plot losses\ndef plot_losses(epochs_seen,tokens_seen,train_losses,val_losses):\n    fig,ax1=plt.subplots(figsize=(5,3))\n    ax1.plot(epochs_seen,train_losses,label=\"training\")\n    ax1.plot(epochs_seen,val_losses,linestyle=\"-.\",label(\"val\"))\n    ax1.set_xlabel(\"epochs\")\n    ax1.set_ylabel(\"loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2=ax1.twiny()\n    ax2.plot(tokens_seen,train_losses,alpha=0)\n    ax2.set_xlabel(\"tokens seen\")\n    fig.tight_layout()\n    plt.savefig(\"loss-plot.pdf\")\n    plt.show()\n\nplot_losses(epochs_seen,tokens_seen,train_losses,val_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:10:21.757910Z","iopub.execute_input":"2025-11-27T18:10:21.758206Z","iopub.status.idle":"2025-11-27T18:10:21.771570Z","shell.execute_reply.started":"2025-11-27T18:10:21.758175Z","shell.execute_reply":"2025-11-27T18:10:21.770117Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_47/1079912594.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    def download_and_load_file():\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"],"ename":"SyntaxError","evalue":"incomplete input (1079912594.py, line 4)","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"Evaluating the Finetuned LLM Using Ollama","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# generate responses and save as json\nfor i,entry in tqdm(enumerate(test_data),total=len(test_data)):\n    input_text=format_input(entry)\n    token_ids=generate(model=model,idx=text_to_token_ids(entry,tokenizer),max_new_tokens=35,\n                  context_size=CONFIGT[\"context_length\"],eos_id=50256)\n    generated_text=token_ids_to_text(token_ids,tokenizer)\n    response_text=generated_text[len(input_text):].replace(\"### Response:\",\"\").strip()\n    test_data[i][\"model_response\"]=response_text\nwith open(\"instruction-data-with-reponse.json\",w) as file:\n    json.dump(test_data,file,indent=4)\n\nprint(test_data[0])\n\n#save the model\n\nimport re\nfile_name=f\"{re.sub(r'[ ()]','',CHOOSE_MODEL) }-sft.pth\"\ntorch.save(model.state_dict(),file_name)\nprint(file_name)\n\n# evaluation - using another larger LLM\nimport psutil\ndef check_if_running(proc_name):\n    running=False\n    for proc in psutil.process_iter([\"name\"]):\n        if proc_name in proc.info[\"name\"]:\n            running=True\n            break\n    return running\n\nollama_running=check_if_running(\"ollama\")\nif not ollama_running:\n    raise RuntimeError(\"Ollama not running.\")\n\nimport urllib.request\ndef query_model(prompt,model=\"llama3\",url=\"http://localhost:11434/api/chat\"):\n    data={\"model\":mo,\n         \"messages\":[{\"role\":\"user\",\"content\":prompt}]}\n    ,\"options\":{\"seed\":123,\"temperature\":0,\"num_ctx:2048\"}\n    payload=json.dumps(data).encode(\"utf-8\")\n    request=urllib.request.Request(url,data=payload,method=\"POST\")\n    request.add_header(\"Content-Type\",\"application/json\")\n    response_data=\"\"\n    with urllib.request.urlopen(request) as response:\n        while True:\n            line=response.readline().decode(\"utf-8\")\n            if not line:\n                break\n            response_json=json.loads(line)\n            response_data+=response_json[\"message\"][\"content\"]\n    return response_data\n    \nresult=query_model(\"what did you deat\",\"llama3\")\nprint(result)\n\n# test\nfor entry in test_data[:3]:\n    prompt=(\n        f\"given the input `{format_input(entry)}`\"\n        f\"and correct output `{entry['output']}`\"\n        f\"score the model response `{entry['model_response']}`\"\n        f\"on a sacle from 0 to 100 where 100 is the best score\"\n    )\n    print(query_model(prompt),entry['output'],entry['model_response'])\n\ndef generate_model_scores(json_data,json_key,model=\"llama3\"):\n    scores=[]\n    for entry in tqdm(json_data,desc=\"Scoring entries\"):\n        prompt=(\n        f\"given the input `{format_input(entry)}`\"\n        f\"and correct output `{entry['output']}`\"\n        f\"score the model response `{entry['model_response']}`\"\n        f\"on a sacle from 0 to 100 where 100 is the best score\"\n        )\n        score=query_model(prompt)\n        try:\n            scores.append(int(score))\n        except ValueError:\n            print(f\"could not convert score {score}\")\n            continue\n    return scores\n\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}